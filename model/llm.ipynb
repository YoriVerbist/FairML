{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4aa96528-45df-4c62-a5c8-84e1de7cfe7a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in /home/yoriv/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.26.2)\n",
      "Requirement already satisfied: pandas in /home/yoriv/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in /home/yoriv/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: langchain in /home/yoriv/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (0.1.11)\n",
      "Requirement already satisfied: transformers in /home/yoriv/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (4.38.2)\n",
      "Collecting Sacremoses (from -r requirements.txt (line 6))\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yoriv/.local/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 2)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yoriv/.local/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 2)) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/yoriv/.local/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 2)) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/yoriv/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/yoriv/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 3)) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/yoriv/.local/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 3)) (3.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/lib/python3/dist-packages (from langchain->-r requirements.txt (line 4)) (5.4.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (3.9.3)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (0.0.25)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (0.1.29)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (0.1.19)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain->-r requirements.txt (line 4)) (8.2.3)\n",
      "Requirement already satisfied: filelock in /home/yoriv/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/yoriv/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (0.21.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/yoriv/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (23.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/yoriv/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/yoriv/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/yoriv/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/yoriv/.local/lib/python3.10/site-packages (from transformers->-r requirements.txt (line 5)) (4.66.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from Sacremoses->-r requirements.txt (line 6)) (8.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/yoriv/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/yoriv/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 4)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/yoriv/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/yoriv/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 4)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/yoriv/.local/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 4)) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/yoriv/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 4)) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/yoriv/.local/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 4)) (0.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/yoriv/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r requirements.txt (line 5)) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/yoriv/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers->-r requirements.txt (line 5)) (4.8.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/yoriv/.local/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain->-r requirements.txt (line 4)) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/yoriv/.local/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.29->langchain->-r requirements.txt (line 4)) (4.2.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/yoriv/.local/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain->-r requirements.txt (line 4)) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/yoriv/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 4)) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/yoriv/.local/lib/python3.10/site-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 4)) (2.16.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/yoriv/.local/lib/python3.10/site-packages (from requests<3,>=2->langchain->-r requirements.txt (line 4)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 4)) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 4)) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2->langchain->-r requirements.txt (line 4)) (2020.6.20)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/yoriv/.local/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 4)) (3.0.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/yoriv/.local/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/yoriv/.local/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain->-r requirements.txt (line 4)) (1.2.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/yoriv/.local/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 4)) (1.0.0)\n",
      "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: libtorrent 2.0.5-build-libtorrent-rasterbar-qrM5vM-libtorrent-rasterbar-2.0.5-bindings-python has a non-standard version number. pip 24.1 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of libtorrent or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: Sacremoses\n",
      "Successfully installed Sacremoses-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07448214-3814-42bf-9ed1-57b75a26643c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import pipeline, set_seed\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43e52d3a-d0e2-433a-ad33-60e71ee8ea1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodels/llama/tokenizer.model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_path)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:1330\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1330\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/import_utils.py:1318\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1316\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n\u001b[1;32m   1317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[0;32m-> 1318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[0;31mImportError\u001b[0m: \nLlamaTokenizer requires the SentencePiece library but it was not found in your environment. Checkout the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "model_path = \"models/llama/tokenizer.model\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f154f48-fd67-4826-a171-e4113478611d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [21:39<00:00, 649.88s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:52<00:00, 26.09s/it]\n",
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the disk and cpu.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: I liked \"Breaking Bad\" and \"Band of Brothers\". Do you have any recommendations of other shows I might like?\n",
      "\n",
      "Comment: Sure! If you enjoyed \"Breaking Bad\" and \"Band of Brothers,\" here are some other shows you might enjoy:\n",
      "\n",
      "1. \"The Sopranos\" - This HBO series is a crime drama that follows the life of a New Jersey mob boss, Tony Soprano, as he navigates the criminal underworld and deals with personal and family issues.\n",
      "2. \"The Wire\" - This HBO series explores the drug trade in Baltimore from multiple perspectives, including law enforcement, drug dealers, and politicians. It's known for its gritty realism and complex characters.\n",
      "3. \"Mad Men\" - This AMC series is set in the 1960s and follows the lives of advertising executives on Madison Avenue\n"
     ]
    }
   ],
   "source": [
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24515316-68e3-41ff-8883-1d24d5d55ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Can you explain what cancer is?\n",
      "\n",
      "Yes, of course! Cancer is a group of diseases that are characterized by the uncontrolled growth and spread of abnormal cells. Cancer can affect any part of the body and can be caused by a variety of factors, including genetic mutations, environmental exposures, and lifestyle choices.\n",
      "\n",
      "Normally, cells in the body grow and divide in a controlled manner, replacing old or damaged cells with new ones. However, in cancer, this process goes awry and cells begin to grow and divide uncontrollably, forming masses called tumors.\n",
      "\n",
      "As tumors grow, they can invade surrounding tissues and organs, disrupting their normal function. Cancer cells can also break away from the primary tumor and spread to other parts of the body through the bloodstream or lymphatic system, a process called metastasis.\n",
      "\n",
      "There are over \n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'Can you explain what cancer is?\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094cd551-ac9a-4489-a93b-df85c7da8bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
